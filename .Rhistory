library(vroom)
library(readr)
library(stringr)
abnb <- read_csv("/Users/tylersmith/Development/Projects/doterra/Airbnb_Open_Data.csv")
# Replace spaces with underscores in column names
names(abnb) <- gsub(" ", "_", names(abnb))
# Filter rows where 'house_rules' mentions 'pets'
pets <- abnb %>%
filter(str_detect(house_rules, "no pets"))
# View the filtered data
head(pets)
# Filter rows where 'house_rules' mentions 'pets'
no_pets <- abnb %>%
filter(str_detect(house_rules, "no pets"))
view(no_pets)
install.packages('doParallel')
library(readr)
nfl_suspensions_data <- read_csv("Downloads/nfl-suspensions-data.csv")
View(nfl_suspensions_data)
library(tidvyerse)
nfl_suspensions <- read_csv("Downloads/nfl-suspensions-data.csv")
suspensions_per_year <- nfl_suspensions %>%
group_by(year) %>%
summarise(count = n())
print(suspensions_per_year)
getwd()
library(tidvyerse)
library(tidvyerse)
install.packages('tidyverser')
install.packages('tidyverse')
install.packages("tidyverse")
library(tidvyerse)
library(tidyverse)
library(vroom)
library(readr)
library(stringr)
abnb <- read_csv("/Users/tylersmith/Development/Projects/doterra/Airbnb_Open_Data.csv")
# Replace spaces with underscores in column names
names(abnb) <- gsub(" ", "_", names(abnb))
abnb <- abnb %>%
select(id, host_id, host_id, host_name, host_identity_verified, neighbourhood_group, neighbourhood, lat, long, room_type,
Construction_year, price) %>%
filter(host_identity_verified == 'verified')
# Transform the price column and convert Construction_year to factor
abnb <- abnb %>%
mutate(price = str_replace_all(price, "\\$", "") %>%    # Replace dollar signs
str_replace_all(",", "") %>%               # Replace commas
as.numeric()) %>%
mutate(Construction_year = as.factor(Construction_year))
# Transform the price column
abnb <- abnb %>%
mutate(price = str_replace_all(price, "\\$", "") %>%    # Replace dollar signs
str_replace_all(",", "") %>%               # Replace commas
as.numeric())
# Count NA's in columns
na_count <- colSums(is.na(abnb))
abnb <- abnb %>%
drop_na(neighbourhood_group, neighbourhood, lat, long)
# Check to make sure that the NA's are removed from the groups that I wanted
sum(is.na(abnb$neighbourhood_group))
sum(is.na(abnb$neighbourhood))
sum(is.na(abnb$lat))
sum(is.na(abnb$long))
unique(abnb$neighbourhood_group)
# Change misspelled values in neighbourhood_group to make sure they match the real value
abnb <- abnb %>%
mutate(neighbourhood_group = case_when(
neighbourhood_group == "brookln" ~ "Brooklyn",
neighbourhood_group == "manhatan" ~ "Manhattan",
TRUE ~ neighbourhood_group # This makes sure that other values are unchanged
))
unique(abnb$neighbourhood_group)
view(abnb)
view(abnb)
view(abnb2)
library(readr)
nfl_suspensions_data <- read_csv("Development/Projects/nfl-suspensions-data.csv")
View(nfl_suspensions_data)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
nfl_suspensions
nfl_suspensions %>%
filter(year == 2012)
library(tidyverse)
nfl_suspensions %>%
filter(year == 2012)
view(nfl_suspensions %>%
filter(year == 2012))
len(nfl_suspensions %>%
filter(year == 2012))
filter(year == 2012))
length(nfl_suspensions %>%
filter(year == 2012))
nfl_suspensions %>%
filter(year == 2012)
library(readr)
nfl_suspensions_data <- read_csv("Development/Projects/nfl-suspensions-data.csv")
View(nfl_suspensions_data)
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
view(nfl_suspensions)
all_years <- tibble(Year = 1986:2014)
nfl_data_complete <- all_years %>%
left_join(nfl_data, by = "Year") %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = "Year") %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = "year") %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(Year = 1986:2015)
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = c("Year" = "Year")) %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(Year = 1986:2015)
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = c("year" = "year")) %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(year = 1986:2015)
nfl_data_complete <- all_years %>%
left_join(nfl_suspensions, by = c("year" = "year")) %>%
mutate(Suspensions = ifelse(is.na(Suspensions), 0, Suspensions))
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(Year = 1986:2015)
nfl_suspensions_2 <- all_years %>%
left_join(nfl_suspensions, by = "year") %>%
group_by(year) %>%
summarize(suspensions_count = n())
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(year = 1986:2015)
nfl_suspensions_2 <- all_years %>%
left_join(nfl_suspensions, by = "Year") %>%
group_by(year) %>%
summarize(suspensions_count = n())
library(tidyverse)
nfl_suspensions <- read_csv("Development/Projects/nfl-suspensions-data.csv")
all_years <- tibble(year = 1986:2015)
nfl_suspensions_2 <- all_years %>%
left_join(nfl_suspensions, by = "year") %>%
group_by(year) %>%
summarize(suspensions_count = n())
write.csv(nfl_suspensions_2, "nfl_supsensions_2.csv", row.names = FALSE)
view(nfl_suspensions_2)
nfl_suspensions_updated <- nfl_suspensions %>%
left_join(nfl_suspensions_2, by = "year")
view(nfl_suspensions_updated)
view(nfl_data_updated)
nfl_data_updated <- nfl_data %>%
full_join(nfl_suspensions_2, by = "year")
nfl_suspensions_updated <- nfl_suspensions %>%
full_join(nfl_suspensions_2, by = "year")
nfl_suspensions_updated$games[is.na(nfl_suspensions_updated$games)] <- 0
unique(nfl_suspensions_updated$Year)
unique(nfl_suspensions_updated$year)
head(gg)
head(ggg)
train <- vroom("./train.csv")
#################################
# ggg Analysis
#################################
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(ranger)
library(rpart)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
library(themis)
library(stacks)
train <- vroom("./train.csv")
setwd("~/Desktop/STAT348/ggg")
train <- vroom("./train.csv")
train <- vroom("./train.csv")
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(ranger)
library(rpart)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
library(themis)
library(stacks)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train_na <- vroom("./trainWithMissingValues")
view(train_na)
train_na <- vroom("./trainWithMissingValues")
getwd()
#################################
# ggg Analysis
#################################
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(ranger)
library(rpart)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
library(themis)
library(stacks)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train_na <- vroom("./trainWithMissingValues")
train_na <- vroom("./trainWithMissingValues.")
train_na <- vroom("./trainWithMissingValues.csv")
view(train_na)
columns_with_na <- names(train_na)[which(colSums(is.na(train_na)) > 0)]
print(columns_with_na)
view(train_na)
na_recipe <- recipe(type ~, train_na) %>%
na_recipe <- recipe(type ~., train_na) %>%
step_impute(bone_length, rotting_flesh, hair_length)
na_recipe <- recipe(type ~., train_na) %>%
step_impute_mean(bone_length, rotting_flesh, hair_length)
baked <- bake(prepped_na_recipe, new_data = na_recipe)
prepped_na_recipe <- prep(my_recipe)
prepped_na_recipe <- prep(train_na_recipe)
prepped_na_recipe <- prep(na_recipe)
baked <- bake(na_recipe, new_data = train_na)
baked
#################################
# ggg Analysis
#################################
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(ranger)
library(rpart)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
library(themis)
library(stacks)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
train_na <- vroom("./trainWithMissingValues.csv")
columns_with_na <- names(train_na)[which(colSums(is.na(train_na)) > 0)]
print(columns_with_na)
na_recipe <- recipe(type ~., train_na) %>%
step_impute_mean(bone_length, rotting_flesh, hair_length)
prepped_na_recipe <- prep(na_recipe)
baked <- bake(na_recipe, new_data = train_na)
baked <- bake(prepped_na_recipe, new_data = train_na)
baked
rmse_vec(trainSet[is.na(train_na)], imputedSet[is.na(train_na)])
rmse_vec(train[is.na(train_na)], baked[is.na(train_na)])
setwd("~/Desktop/STAT348/ggg")
getwd()
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(ranger)
library(rpart)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
library(themis)
library(stacks)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
view(train)
my_recipe <- recipe(type ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_lencode_mixed(all_nominal_predictors(), outcome = vars(type))
knn_model <- nearest_neighbor(neighbors=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kknn")
knn_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(knn_model)
# cross validation
knn_tuning_grid <- grid_regular(neighbors(),
levels = 5)
knn_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- knn_wf %>%
tune_grid(resamples = knn_folds,
grid = knn_tuning_grid,
metrics = metric_set(roc_auc))
my_recipe <- recipe(type ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
# KNN --------------------------------------------------------------------
knn_model <- nearest_neighbor(neighbors=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kknn")
my_recipe <- recipe(type ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
# KNN --------------------------------------------------------------------
knn_model <- nearest_neighbor(neighbors=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kknn")
#################################
# ggg Analysis
#################################
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(ranger)
library(rpart)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
library(themis)
library(stacks)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
# train_na <- vroom("./trainWithMissingValues.csv")
columns_with_na <- names(train_na)[which(colSums(is.na(train_na)) > 0)]
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
library(ranger)
library(rpart)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
library(themis)
library(stacks)
train <- vroom("./train.csv")
test <- vroom("./test.csv")
my_recipe <- recipe(type ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) # turn all numeric features into factors
knn_model <- nearest_neighbor(neighbors=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kknn")
knn_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(knn_model)
# cross validation
knn_tuning_grid <- grid_regular(neighbors(),
levels = 5)
knn_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- knn_wf %>%
tune_grid(resamples = knn_folds,
grid = knn_tuning_grid,
metrics = metric_set(roc_auc))
my_recipe <- recipe(type ~ ., data = train) %>%
step_rm(id) %>%
step_normalize(all_numberic_predictors()) %>%
step_dummy(color)
knn_model <- nearest_neighbor(neighbors=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kknn")
knn_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(knn_model)
# cross validation
knn_tuning_grid <- grid_regular(neighbors(),
levels = 5)
knn_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- knn_wf %>%
tune_grid(resamples = knn_folds,
grid = knn_tuning_grid,
metrics = metric_set(roc_auc))
my_recipe <- recipe(type ~ ., data = train) %>%
step_rm(id) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(color)
knn_model <- nearest_neighbor(neighbors=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kknn")
knn_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(knn_model)
# cross validation
knn_tuning_grid <- grid_regular(neighbors(),
levels = 5)
knn_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- knn_wf %>%
tune_grid(resamples = knn_folds,
grid = knn_tuning_grid,
metrics = metric_set(roc_auc))
knn_bestTune <- CV_results %>%
select_best("roc_auc")
# finalize workflow
final_knn_wf <- knn_wf %>%
finalize_workflow(knn_bestTune) %>%
fit(data = train)
predict_and_format <- function(model, newdata, filename){
predictions <- predict(model, new_data = newdata)
submission <- predictions %>%
mutate(id = test$id) %>%
rename("type" = ".pred_class") %>%
select(2,1)
vroom_write(submission, filename, delim = ',')
}
predict_and_format(final_knn_wf, test, "./knn_preds.csv")
rand_forest_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees=500) %>% # or 1000
set_engine("ranger") %>%
set_mode("classification")
rand_forest_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(rand_forest_mod)
rand_forest_tuning_grid <- grid_regular(mtry(range = c(1, (ncol(train)-1))),
min_n(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
forest_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- rand_forest_workflow %>%
tune_grid(resamples = forest_folds,
grid = rand_forest_tuning_grid,
metrics = metric_set(roc_auc)) # f_meas, sens, recall, spec, precision, accuracy
## Find Best Tuning Parameters
forest_bestTune <- CV_results %>%
select_best("roc_auc")
## Finalize the Workflow & fit it
final_forest_wf <- rand_forest_workflow %>%
finalize_workflow(forest_bestTune) %>%
fit(data = train)
predict_and_format(final_forest_wf, test, "./rand_forest_preds.csv")
# Naive Bayes -------------------------------------------------------------
nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
set_mode('classification') %>%
set_engine('naivebayes')
nb_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(nb_model)
# Tune smoothness and Laplace here
nb_tuning_grid <- grid_regular(Laplace(),
smoothness(),
levels = 5)
## Split data for CV
nb_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- nb_wf %>%
tune_grid(resamples = nb_folds,
grid = nb_tuning_grid,
metrics = metric_set(roc_auc)) # f_meas, sens, recall, spec, precision, accuracy
## Find Best Tuning Parameters
nb_bestTune <- CV_results %>%
select_best("roc_auc")
## Finalize the Workflow & fit it
final_nb_wf <- nb_wf %>%
finalize_workflow(nb_bestTune) %>%
fit(data = train)
# Predict
predict(final_nb_wf, new_data = test, type = 'prob')
predict_and_format(final_nb_wf, test, "./nb_preds.csv")
# SVM ---------------------------------------------------------------------
svmRadial <- svm_rbf(rbf_sigma = tune(), cost = tune()) %>%
set_mode('classification') %>%
set_engine('kernlab')
svm_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(svmRadial)
# cross validation
svm_tuning_grid <- grid_regular(cost(),
rbf_sigma(),
levels = 5)
svm_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- svm_wf %>%
tune_grid(resamples = svm_folds,
grid = svm_tuning_grid,
metrics = metric_set(roc_auc))
install.packages('kernlab')
library(kernlab)
# SVM ---------------------------------------------------------------------
svmRadial <- svm_rbf(rbf_sigma = tune(), cost = tune()) %>%
set_mode('classification') %>%
set_engine('kernlab')
svm_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(svmRadial)
# cross validation
svm_tuning_grid <- grid_regular(cost(),
rbf_sigma(),
levels = 5)
svm_folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
CV_results <- svm_wf %>%
tune_grid(resamples = svm_folds,
grid = svm_tuning_grid,
metrics = metric_set(roc_auc))
svm_bestTune <- CV_results %>%
select_best("roc_auc")
# finalize workflow
final_svm_wf <- svm_wf %>%
finalize_workflow(svm_bestTune) %>%
fit(data = train)
predict_and_format(final_svm_wf, test, "./svm_radial_preds.csv")
